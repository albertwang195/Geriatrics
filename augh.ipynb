{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4b91305c",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Project Code\"\n",
    "subtitle: Team name\n",
    "author: Author 1, Author 2, Author 3, and Author 4 \n",
    "date: 02/27/2023\n",
    "number-sections: true\n",
    "abstract: _This file contains the code for the project on <>, as part of the STAT303-2 course in Winter 2023_.\n",
    "format: \n",
    "  html:\n",
    "    toc: true\n",
    "    toc-title: Contents\n",
    "    self-contained: true\n",
    "    font-size: 100%\n",
    "    toc-depth: 4\n",
    "    mainfont: serif\n",
    "jupyter: python3\n",
    "---\n",
    "\n",
    "## Length of the code {-}\n",
    "No restriction\n",
    "\n",
    "**Delete this section from the report, when using this template.** \n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.formula.api as smf\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import itertools\n",
    "import time\n",
    "\n",
    "## Data quality check / cleaning / preparation \n",
    "\n",
    "Put code with comments. The comments should explain the code such that it can be easily understood. You may put text *(in a markdown cell)* before a large chunk of code to explain the overall purpose of the code, if it is not intuitive. **Put the name of the person / persons who contributed to each code chunk / set of code chunks.** An example is given below.\n",
    "\n",
    "### Data quality check\n",
    "*By Elton John*\n",
    "\n",
    "The code below visualizes the distribution of all the variables in the dataset, and their association with the response.\n",
    "\n",
    "### Data cleaning\n",
    "*By NAMES*\n",
    "\n",
    "From the data quality check we realized that:\n",
    "\n",
    "1. Some of the columns that should have contained only numeric values, specifically <>, <>, and <> have special characters such as \\*, #, %. We'll remove these characters, and convert the datatype of these columns to numeric.\n",
    "\n",
    "2. Some of the columns have more than 60% missing values, and it is very difficult to impute their values, as the values seem to be missing at random with negligible association with other predictors. We'll remove such columns from the data.\n",
    "\n",
    "3. The column `number_of_bedrooms` has some unreasonably high values such as 15. As our data consist of single-family homes in Evanston, we suspect that any value greater than 5 may be incorrect. We'll replace all values that are greater than 5 with an estimate obtained using the $K$-nearest neighbor approach.\n",
    "\n",
    "4. The columns `house_price` has some unreasonably high values. We'll tag all values greater than 1 billion dollars as \"potentially incorrect observation\", to see if they distort our prediction / inference later on.\n",
    "\n",
    "The code below implements the above cleaning.\n",
    "\n",
    "#...Code with comments...#\n",
    "\n",
    "### Data preparation\n",
    "*By Kegan Grace and Albert Wang*\n",
    "\n",
    "Our dataset was a collection of csv files for each decade from 1950-2019, so we needed to concatenate all the files together to expand our dataset and number of observations.\n",
    "\n",
    "y1950 = pd.read_csv('1950.xls')\n",
    "y1960 = pd.read_csv('1960.xls')\n",
    "y1970 = pd.read_csv('1970.xls')\n",
    "y1980 = pd.read_csv('1980.xls')\n",
    "y1990 = pd.read_csv('1990.xls')\n",
    "y2000 = pd.read_csv('2000.xls')\n",
    "y2010 = pd.read_csv('2010.xls')\n",
    "all_data = pd.concat([y1950,y1960,y1970,y1980,y1990,y2000,y2010])\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X = all_data.drop('pop', axis=1)\n",
    "y = all_data['pop']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "train = pd.concat([X_train, y_train], axis=1)\n",
    "test = pd.concat([X_test, y_test], axis=1)\n",
    "\n",
    "## Exploratory data analysis\n",
    "\n",
    "Put code with comments. The comments should explain the code such that it can be easily understood. You may put text *(in a markdown cell)* before a large chunk of code to explain the overall purpose of the code, if it is not intuitive. **Put the name of the person / persons who contributed to each code chunk / set of code chunks.**\n",
    "\n",
    "### Histogram of Popularity Values\n",
    "*By Albert Wang*\n",
    "\n",
    "#histogram/dist of song popularity values\n",
    "sns.displot(df['pop'], bins=30)\n",
    "plt.show()\n",
    "\n",
    "### Numerical variables against song popularity\n",
    "*By Grant Li*\n",
    "\n",
    "#visualizations of all numerical variables (including non-audial features) agaisnt song popularity\n",
    "predictors = all_data.corr().columns\n",
    "fig, axes = plt.subplots(4,3,figsize=(20,20))\n",
    "p = 0\n",
    "for i in range(4):\n",
    "    for j in range(3):\n",
    "        \n",
    "        if p >= 12:\n",
    "            break\n",
    "        predictor = predictors[p]\n",
    "        \n",
    "        sns.scatterplot(ax=axes[i,j], x=predictor,y='pop',data=all_data)\n",
    "        p+=1\n",
    "plt.show()\n",
    "\n",
    "## Developing the model\n",
    "\n",
    "Put code with comments. The comments should explain the code such that it can be easily understood. You may put text *(in a markdown cell)* before a large chunk of code to explain the overall purpose of the code, if it is not intuitive. **Put the name of the person / persons who contributed to each code chunk / set of code chunks.**\n",
    "\n",
    "### Checking Multicollinearity\n",
    "*By Grant Li*\n",
    "\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from statsmodels.tools.tools import add_constant\n",
    "\n",
    "X = train[['dB', 'dnce', 'nrgy', 'acous', 'dur', 'val', 'spch', 'live']]\n",
    "\n",
    "X = add_constant(X)\n",
    "vif_data = pd.DataFrame()\n",
    "vif_data[\"feature\"] = X.columns\n",
    "\n",
    "for i in range(len(X.columns)):\n",
    "    vif_data.loc[i,'VIF'] = variance_inflation_factor(X.values, i)\n",
    "\n",
    "print(vif_data)\n",
    "\n",
    "### Model Testing\n",
    "*By Nate Kim*\n",
    "\n",
    "# initial model\n",
    "model = smf.ols(formula='pop~acous+dB+dur+dnce+nrgy+val', data=train).fit()\n",
    "print(model.summary())\n",
    "#test MAE\n",
    "pred_pop = model.predict(test)\n",
    "mae = (np.abs(test['pop'] - pred_pop)).mean()\n",
    "print(\"Test MAE is\", mae)\n",
    "#train MAE\n",
    "mae_train = (np.abs(train['pop'] - pred_pop)).mean()\n",
    "print(\"Train MAE is\",mae_train)\n",
    "\n",
    "#model with **2 transformation on duration\n",
    "model = smf.ols(formula='pop~acous+dB+dur+dnce+nrgy+I(dur**2)', data=train).fit()\n",
    "print(model.summary())\n",
    "#test MAE\n",
    "pred_pop = model.predict(test)\n",
    "mae = (np.abs(test['pop'] - pred_pop)).mean()\n",
    "print(\"Test MAE is\", mae)\n",
    "#train MAE\n",
    "mae_train = (np.abs(train['pop'] - pred_pop)).mean()\n",
    "print(\"Train MAE is\",mae_train)\n",
    "\n",
    "#model with log transformation on duration\n",
    "model = smf.ols(formula='pop~acous+dB+dur+dnce+nrgy+I(np.log(dur))', data=train).fit()\n",
    "print(model.summary())\n",
    "#test MAE\n",
    "pred_pop = model.predict(test)\n",
    "mae = (np.abs(test['pop'] - pred_pop)).mean()\n",
    "print(\"Test MAE is\", mae)\n",
    "#train MAE\n",
    "mae_train = (np.abs(train['pop'] - pred_pop)).mean()\n",
    "print(\"Train MAE is\",mae_train)\n",
    "\n",
    "# model with sqrt tranformation on dur\n",
    "model = smf.ols(formula='pop~acous+dB+dur+dnce+nrgy+I(np.sqrt(dur))', data=train).fit()\n",
    "print(model.summary())\n",
    "#test MAE\n",
    "pred_pop = model.predict(test)\n",
    "mae = (np.abs(test['pop'] - pred_pop)).mean()\n",
    "print(\"Test MAE is\", mae)\n",
    "#train MAE\n",
    "mae_train = (np.abs(train['pop'] - pred_pop)).mean()\n",
    "print(\"Train MAE is\",mae_train)\n",
    "\n",
    "\n",
    "\n",
    "### Code fitting the final model\n",
    "\n",
    "model_best = smf.ols(formula='pop~acous+dB+dur+nrgy*dnce+val+I(np.log(dur))', data=train).fit()\n",
    "print(model_best.summary())\n",
    "#test MAE\n",
    "pred_pop = model_best.predict(test)\n",
    "mae = (np.abs(test['pop'] - pred_pop)).mean()\n",
    "print(\"Test MAE is\", mae)\n",
    "#train MAE\n",
    "mae_train = (np.abs(train['pop'] - pred_pop)).mean()\n",
    "print(\"Train MAE is\",mae_train)\n",
    "\n",
    "Put the code(s) that fit the final model(s) in separate cell(s), i.e., the code with the `.ols()` or `.logit()` functions.\n",
    "\n",
    "## Conclusions and Recommendations to stakeholder(s)\n",
    "\n",
    "You may or may not have code to put in this section. Delete this section if it is irrelevant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "647de3e0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
